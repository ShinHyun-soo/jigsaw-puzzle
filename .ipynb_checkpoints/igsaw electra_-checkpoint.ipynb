{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import lightning as L\n",
    "\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from torchvision.io import read_image\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b55116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify vit-g (add pos bias, attn bias)\n",
    "from timm.models.vision_transformer import Block, Attention, VisionTransformer\n",
    "\n",
    "def attention_forward(self, x, attn_bias=None):\n",
    "    B, N, C = x.shape\n",
    "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv.unbind(0)\n",
    "    q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "    q = q * self.scale\n",
    "    attn = q @ k.transpose(-2, -1)\n",
    "    if attn_bias is not None:\n",
    "        attn + attn_bias\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1, 2).reshape(B, N, C)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "    return x\n",
    "Attention.forward = attention_forward\n",
    "\n",
    "def block_forward(self, x_and_attn_bias):\n",
    "    x, attn_bias = x_and_attn_bias\n",
    "    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_bias)))\n",
    "    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "    return (x, attn_bias)\n",
    "Block.forward = block_forward\n",
    "\n",
    "def vision_transformer_forward_features(self, x, embed_bias=None, attn_bias=None):\n",
    "    x = self.patch_embed(x)\n",
    "    x = self._pos_embed(x)\n",
    "    if embed_bias is not None:\n",
    "        x = x + embed_bias\n",
    "    x = self.patch_drop(x)\n",
    "    x = self.norm_pre(x)\n",
    "    x, _ = self.blocks((x,attn_bias))\n",
    "    x = self.norm(x)\n",
    "    return x\n",
    "VisionTransformer.forward_features = vision_transformer_forward_features\n",
    "\n",
    "def vision_transformer_forward(self, x, embed_bias=None, attn_bias=None):\n",
    "    x = self.forward_features(x, embed_bias, attn_bias)\n",
    "    return x\n",
    "VisionTransformer.forward = vision_transformer_forward\n",
    "\n",
    "model = timm.create_model('vit_medium_patch16_gap_256', pretrained=True, num_classes=0)\n",
    "\n",
    "model_config = {\n",
    "    'image_size':256,\n",
    "    'patch_size':16,\n",
    "    'hidden_size':512,\n",
    "    'num_attention_heads':8,\n",
    "}\n",
    "\n",
    "transform_config = timm.data.resolve_data_config(model.pretrained_cfg)\n",
    "transform_config.pop('crop_pct')\n",
    "transform_config.pop('crop_mode')\n",
    "\n",
    "transform = timm.data.create_transform(\n",
    "    **transform_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['seed']=42\n",
    "config['batch_size']=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d889f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train1.csv')\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=config['seed'])\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, df, data_path, mode='train'):\n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path']))\n",
    "            shuffle_order = row[[str(i) for i in range(1, 17)]].values-1\n",
    "            image_src = self.reset_image(image, shuffle_order)\n",
    "            image_reshuffle, reshuffle_order = self.shuffle_image(image_src)\n",
    "            adjacency_matrix = self.get_adjacency_matrix(reshuffle_order)\n",
    "            data = {\n",
    "                'image_src':image_src,\n",
    "                'image_reshuffle':image_reshuffle,\n",
    "                'order':reshuffle_order,\n",
    "                'adjacency_matrix':adjacency_matrix,\n",
    "                'score': self.get_score(range(16), reshuffle_order),\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'val':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path'])).numpy()\n",
    "            shuffle_order = row[[str(i) for i in range(1, 17)]].values-1\n",
    "            adjacency_matrix = self.get_adjacency_matrix(shuffle_order.tolist())\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'order':shuffle_order,\n",
    "                'adjacency_matrix':adjacency_matrix,\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'inference':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(os.path.join(self.data_path, row['img_path'])).numpy()\n",
    "            data = {\n",
    "                'image':image\n",
    "            }\n",
    "            return data\n",
    "\n",
    "    def reset_image(self, image, shuffle_order):\n",
    "        c, h, w = image.shape\n",
    "        block_h, block_w = h//4, w//4\n",
    "        image_src = [[0 for _ in range(4)] for _ in range(4)]\n",
    "        for idx, order in enumerate(shuffle_order):\n",
    "            h_idx, w_idx = divmod(order,4)\n",
    "            h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "            image_src[h_idx][w_idx] = image[:, block_h * h_idx_shuffle : block_h * (h_idx_shuffle+1), block_w * w_idx_shuffle : block_w * (w_idx_shuffle+1)]\n",
    "        image_src = np.concatenate([np.concatenate(image_row, -1) for image_row in image_src], -2)\n",
    "        return image_src\n",
    "\n",
    "    def shuffle_image(self, image):\n",
    "        c, h, w = image.shape\n",
    "        block_h, block_w = h//4, w//4\n",
    "        shuffle_order = list(range(0, 16))\n",
    "        random.shuffle(shuffle_order)\n",
    "        image_shuffle = [[0 for _ in range(4)] for _ in range(4)]\n",
    "        for idx, order in enumerate(shuffle_order):\n",
    "            h_idx, w_idx = divmod(order,4)\n",
    "            h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "            image_shuffle[h_idx_shuffle][w_idx_shuffle] = image[:, block_h * h_idx : block_h * (h_idx+1), block_w * w_idx : block_w * (w_idx+1)]\n",
    "        image_shuffle = np.concatenate([np.concatenate(image_row, -1) for image_row in image_shuffle], -2)\n",
    "        return image_shuffle, shuffle_order\n",
    "\n",
    "    def get_adjacency_matrix(self, order): # 패치에 대하여 연결된 패치 찾기\n",
    "        order_matrix = [order[4*i:4*(i+1)]for i in range(4)]\n",
    "        adj_matrix = np.zeros((16,16), dtype=int)\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                o = order_matrix[i][j]\n",
    "                i_o, j_o = divmod(o,4)\n",
    "                for i_add,j_add in [(-1,0), (1,0), (0,1), (0,-1)]:\n",
    "                    i_compare, j_compare = i_o+i_add, j_o+j_add\n",
    "                    if i_compare<0 or i_compare>=4 or j_compare<0 or j_compare>=4 : continue\n",
    "                    o_compare = order[i_compare*4+j_compare]\n",
    "                    i_, j_ = i*4+j, order.index(i_compare*4+j_compare)\n",
    "                    if (i_add,j_add) == (-1,0):\n",
    "                        adj_matrix[i_][j_] = 1 # 상\n",
    "                        adj_matrix[j_][i_] = 2 # 하\n",
    "                    elif (i_add,j_add) == (-1,0):\n",
    "                        adj_matrix[i_][j_] = 2\n",
    "                        adj_matrix[j_][i_] = 1\n",
    "                    elif  (i_add,j_add) == (0,-1):\n",
    "                        adj_matrix[i_][j_] = 3 # 좌\n",
    "                        adj_matrix[j_][i_] = 4 # 우\n",
    "                    elif (i_add,j_add) == (0,1):\n",
    "                        adj_matrix[i_][j_] = 4\n",
    "                        adj_matrix[j_][i_] = 3\n",
    "        return adj_matrix\n",
    "\n",
    "    def get_score(self, order_true, order_pred): # regression task? 현재 아키텍처와 맞지 않을듯\n",
    "        puzzle_a = np.array(order_true, dtype=int).reshape(4, 4)\n",
    "        puzzle_s = np.array(order_pred, dtype=int).reshape(4, 4)\n",
    "\n",
    "        accuracies = {}\n",
    "        accuracies['1x1'] = np.mean(puzzle_a == puzzle_s)\n",
    "\n",
    "        combinations_2x2 = [(i, j) for i in range(3) for j in range(3)]\n",
    "        combinations_3x3 = [(i, j) for i in range(2) for j in range(2)]\n",
    "\n",
    "        for size in range(2, 5):  # Loop through sizes 2, 3, 4\n",
    "            correct_count = 0  # Initialize counter for correct full sub-puzzles\n",
    "            total_subpuzzles = 0\n",
    "            combinations = combinations_2x2 if size == 2 else combinations_3x3 if size == 3 else [(0, 0)]\n",
    "            for start_row, start_col in combinations:\n",
    "                rows = slice(start_row, start_row + size)\n",
    "                cols = slice(start_col, start_col + size)\n",
    "                if np.array_equal(puzzle_a[rows, cols], puzzle_s[rows, cols]):\n",
    "                    correct_count += 1\n",
    "                total_subpuzzles += 1\n",
    "\n",
    "            accuracies[f'{size}x{size}'] = correct_count / total_subpuzzles\n",
    "\n",
    "        score = (accuracies['1x1'] + accuracies['2x2'] + accuracies['3x3'] + accuracies['4x4']) / 4.\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawCollateFn:\n",
    "    def __init__(self, transform, mode):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.mode=='train':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image_reshuffle'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            order = torch.LongTensor([data['order'] for data in batch])\n",
    "            adjacency_matrx = torch.LongTensor([data['adjacency_matrix'] for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'order':order,\n",
    "                'adjacency_matrx':adjacency_matrx\n",
    "            }\n",
    "        elif self.mode=='val':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            order = torch.LongTensor([data['order'] for data in batch])\n",
    "            adjacency_matrx = torch.LongTensor([data['adjacency_matrix'] for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'order':order,\n",
    "                'adjacency_matrx':adjacency_matrx\n",
    "            }\n",
    "        elif self.mode=='inference':\n",
    "            pixel_values = torch.stack([self.transform(Image.fromarray(data['image'].astype(np.uint8).transpose(1,2,0))) for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = JigsawDataset(\n",
    "    df=train_df,\n",
    "    data_path='./data',\n",
    "    mode='train'\n",
    ")\n",
    "val_dataset = JigsawDataset(\n",
    "    df=val_df,\n",
    "    data_path='./data',\n",
    "    mode='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, collate_fn=JigsawCollateFn(transform, 'train'), batch_size=config['batch_size'])\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=JigsawCollateFn(transform, 'val'), batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawElectra(nn.Module):\n",
    "    \"\"\"\n",
    "    1st Stage:\n",
    "    In the initial stage, a transformer architecture is employed to discern optimal patch arrangements for each puzzle segment.\n",
    "    This involves intricate spatial relationships, where the model dynamically identifies neighboring patches in cardinal directions(i.e., up, down, left, right).\n",
    "    The foundation of this stage lies in the incorporation of attention matrices at the final layer, providing nuanced insights into patch interdependencies.\n",
    "    \n",
    "    2nd Stage:\n",
    "    Subsequently, the second stage capitalizes on the predicted matrices from the initial stage to derive piece-type embeddings and connect-type embedding.\n",
    "    These embeddings encapsulate diverse spatial configurations, such as cross shapes, left corners and right, and else.\n",
    "    The innovation lies in the integration of piece-type embeddings as positional embedding biases, enhancing the model's contextual awareness.\n",
    "    Furthermore, connect matrix embeddings serve as attention biases, enabling the model to capture intricate inter-piece relationships.\n",
    "    The final objective of this stage is to predict an optimal reordering sequence, leveraging the acquired embeddings.\n",
    "    \n",
    "    The backbone model shares weights excluding head layers. And losses are jointly computed for gradient updates, aiming for efficient learning and high performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        super(JigsawElectra, self).__init__()\n",
    "        for k,v in config.items():\n",
    "            setattr(self,k,v)\n",
    "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
    "        self.num_patch_per_block = int(self.image_size/4/self.patch_size)\n",
    "        self.model = model\n",
    "        \n",
    "        self.pos_emb = nn.Parameter(torch.randn(16, self.hidden_size))\n",
    "        self.piece_type_emb = nn.Embedding(10, self.hidden_size, padding_idx=0)\n",
    "        self.piece_type_emb.weight.data[0,:]=0\n",
    "        self.piece_type_emb.weight.data = self.piece_type_emb.weight.data*0.1\n",
    "        self.connect_type_emb = nn.Embedding(5, self.num_attention_heads, padding_idx=0)\n",
    "        self.connect_type_emb.weight.data[0,:]=0\n",
    "        self.connect_type_emb.weight.data = self.connect_type_emb.weight.data*0.1\n",
    "        \n",
    "        self.local_linear1 = nn.LazyLinear(self.hidden_size)\n",
    "        self.local_linear2 = nn.LazyLinear(self.hidden_size)\n",
    "        self.local_conv = nn.Conv2d(self.num_attention_heads, self.num_attention_heads, int(self.image_size/16), int(self.image_size/16))\n",
    "        self.local_clf = nn.Sequential(\n",
    "            nn.LazyLinear(self.num_attention_heads),\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(5),\n",
    "        )\n",
    "\n",
    "        self.global_conv = nn.Conv1d(self.hidden_size, self.hidden_size, int(self.image_size/16), int(self.image_size/16))\n",
    "        self.global_clf = nn.Sequential(\n",
    "            nn.LazyLinear(self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(16),\n",
    "        )\n",
    "\n",
    "    def _transpose(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        b, h, l, d = x.shape\n",
    "        x = torch.cat(x.reshape(b, h, -1, self.num_patch_per_block, d).split(self.num_patch_per_block, 2), 3).reshape(b, h, l, d)\n",
    "        return x\n",
    "        \n",
    "    def local_forward(self, x, label=None):\n",
    "        pos_emb = self.pos_emb.reshape(4,4,-1)\n",
    "        pos_emb = pos_emb.unsqueeze(-2).repeat(1,1,self.num_patch_per_block,1).reshape(4,-1,self.hidden_size)\n",
    "        pos_emb = pos_emb.unsqueeze(1).repeat(1,self.num_patch_per_block, 1, 1).reshape(-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "        pos_emb = pos_emb.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        x = self.model(x, embed_bias=pos_emb)\n",
    "        x1 = self._transpose(self.local_linear1(x))\n",
    "        x2 = self._transpose(self.local_linear2(x))\n",
    "        x = torch.matmul(x1,x2.transpose(-1, -2)).transpose(-1,-2)\n",
    "        x = self.local_conv(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = self.local_clf(x)\n",
    "        probs = nn.Softmax(dim=-1)(x)\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x.reshape(-1, 5), label.reshape(-1))\n",
    "        return x, probs, loss\n",
    "        \n",
    "    def global_forward(self, x, piece_type=None, connect_type=None, label=None):\n",
    "        pos_emb = self.pos_emb.reshape(4,4,-1)\n",
    "        pos_emb = pos_emb.unsqueeze(-2).repeat(1,1,self.num_patch_per_block,1).reshape(4,-1,self.hidden_size)\n",
    "        pos_emb = pos_emb.unsqueeze(1).repeat(1,self.num_patch_per_block, 1, 1).reshape(-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "        pos_emb = pos_emb.reshape(-1, self.hidden_size)\n",
    "        \n",
    "        if piece_type is not None:\n",
    "            b = piece_type.shape[0]\n",
    "            piece_emb = self.piece_type_emb(piece_type).reshape(b, 4, 4, -1)\n",
    "            piece_emb = piece_emb.unsqueeze(-2).repeat(1,1,1,self.num_patch_per_block,1).reshape(b, 4,-1,self.hidden_size)\n",
    "            piece_emb = piece_emb.unsqueeze(2).repeat(1,1,self.num_patch_per_block, 1, 1).reshape(b,-1, 4*self.num_patch_per_block, self.hidden_size)\n",
    "            piece_emb = piece_emb.reshape(b,-1, self.hidden_size)\n",
    "            pos_emb = piece_emb+pos_emb\n",
    "            \n",
    "        attn_bias = None\n",
    "        if connect_type is not None:\n",
    "            b = connect_type.shape[0]\n",
    "            attn_bias = self.connect_type_emb(connect_type) # B 16,16,8\n",
    "            attn_bias = attn_bias.unsqueeze(-2).repeat(1,1,1,int(self.image_size/16),1).reshape(b,16,-1,self.num_attention_heads)\n",
    "            attn_bias = attn_bias.unsqueeze(2).repeat(1,1,int(self.image_size/16), 1, 1).reshape(b,-1, self.image_size, self.num_attention_heads)\n",
    "            attn_bias = attn_bias.permute(0,3,1,2)\n",
    "            \n",
    "        x = self.model(\n",
    "            x,\n",
    "            embed_bias=pos_emb,\n",
    "            attn_bias=attn_bias,\n",
    "        )\n",
    "        x = self._transpose(x)\n",
    "        b, h, l, d = x.shape\n",
    "        x = x.permute(0,1,3,2).reshape(b,h*d,l)\n",
    "        x = self.global_conv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.global_clf(x)\n",
    "        probs = nn.Softmax(dim=-1)(x)\n",
    "        \n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x.reshape(-1, 16), label.reshape(-1))\n",
    "        return x, probs, loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitJigsawElectra(L.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.jigsaw_electra = JigsawElectra(model, config)\n",
    "        self.inference_iter = 1\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return opt\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        x_local, x_local_probs, loss_local = self.jigsaw_electra.local_forward(batch['pixel_values'], batch['adjacency_matrx'])        \n",
    "        connect_type = x_local_probs.argmax(-1).detach()\n",
    "        piece_type = self.connect_to_piece(connect_type).detach()\n",
    "        x_global, x_global_probs, loss_global = self.jigsaw_electra.global_forward(batch['pixel_values'], piece_type=piece_type, connect_type=connect_type, label=batch['order'])\n",
    "        loss = loss_local*0.2 + loss_global\n",
    "        self.log(\"train_loss_local\", loss_local, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_loss_global\", loss_global, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x_local, x_local_probs, loss_local = self.jigsaw_electra.local_forward(batch['pixel_values'], batch['adjacency_matrx'])\n",
    "        self.log(\"val_loss_local\", loss_local)\n",
    "        connect_type = x_local_probs.argmax(-1).detach()\n",
    "        piece_type = self.connect_to_piece(connect_type).detach()\n",
    "        local_accuracy = torch.mean(1*(connect_type == batch['adjacency_matrx']), dtype=torch.float32)\n",
    "        self.log(\"val_acc_local\", local_accuracy)\n",
    "        x_global, x_global_probs, loss_global = self.jigsaw_electra.global_forward(batch['pixel_values'], piece_type=piece_type, connect_type=connect_type, label=batch['order'])\n",
    "        self.log(\"val_loss_global\", loss_global)\n",
    "        self.validation_step_outputs.append((x_global_probs, batch['order']))\n",
    "        return\n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        label = batch.get('order', None)\n",
    "        for i in range(self.inference_iter):\n",
    "            x_local, x_local_probs, _ = self.jigsaw_electra.local_forward(pixel_values)        \n",
    "            connect_type = x_local_probs.argmax(-1).detach()\n",
    "            piece_type = self.connect_to_piece(connect_type).detach()\n",
    "            x_global, x_global_probs, _ = self.jigsaw_electra.global_forward(batch['pixel_values'], piece_type=piece_type, connect_type=connect_type)\n",
    "            reorder = self._probs_to_order(x_global_probs)\n",
    "            pixel_values = self._reorder_image(pixel_values, reorder)\n",
    "        return x_global_probs, reorder, label\n",
    "    \n",
    "    def connect_to_piece(self, connect_types):\n",
    "        device = connect_types.device\n",
    "        connect_types = connect_types.detach().cpu()\n",
    "        piece_types = []\n",
    "        for connect_type in connect_types:\n",
    "            piece_type = []\n",
    "            for connect_type_row in connect_type:\n",
    "                connect_bins = torch.bincount(connect_type_row)\n",
    "                if torch.equal(connect_bins[1:5], torch.LongTensor([0,1,0,1])): #  ┌\n",
    "                    piece_type.append(1)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([0,1,1,1])): # ㅜ\n",
    "                    piece_type.append(2)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([0,1,1,0])): # ㄱ\n",
    "                    piece_type.append(3)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,1,0,1])): # ㅏ\n",
    "                    piece_type.append(4)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,1,1,0])): # ㅓ\n",
    "                    piece_type.append(5)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,0,0,1])): # ㄴ\n",
    "                    piece_type.append(6)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,0,1,1])): # ㅗ\n",
    "                    piece_type.append(7)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,0,1,0])): # ┘\n",
    "                    piece_type.append(8)\n",
    "                elif torch.equal(connect_bins[1:5], torch.LongTensor([1,1,1,1])): # +\n",
    "                    piece_type.append(9)\n",
    "                else: # unknown\n",
    "                    piece_type.append(0)\n",
    "            piece_types.append(piece_type)\n",
    "        piece_types = torch.LongTensor(piece_types).to(device)\n",
    "        return piece_types\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        order_pred = []\n",
    "        order_true = []\n",
    "        for probs, order in self.validation_step_outputs:\n",
    "            order_pred.append(self._probs_to_order(probs))\n",
    "            order_true.append(order)\n",
    "        order_pred = torch.cat(order_pred).detach().cpu().numpy()\n",
    "        order_true = torch.cat(order_true).detach().cpu().numpy()\n",
    "        \n",
    "        score, accuracies = self._get_score(order_true, order_pred)\n",
    "\n",
    "        self.log(\"val_score_1x1\", accuracies['1x1'])\n",
    "        self.log(\"val_score\", score)\n",
    "        self.validation_step_outputs.clear()\n",
    "        return\n",
    "    \n",
    "    def _get_score(self, order_true, order_pred):\n",
    "        combinations_2x2 = [(i, j) for i in range(3) for j in range(3)]\n",
    "        combinations_3x3 = [(i, j) for i in range(2) for j in range(2)]\n",
    "        accuracies = {}\n",
    "        accuracies['1x1'] = np.mean(order_true == order_pred)\n",
    "        \n",
    "        for size in range(2, 5): \n",
    "            correct_count = 0  \n",
    "            total_subpuzzles = 0\n",
    "            for i in range(len(order_true)):\n",
    "                puzzle_a = order_true[i].reshape(4, 4)\n",
    "                puzzle_s = order_pred[i].reshape(4, 4)\n",
    "                combinations = combinations_2x2 if size == 2 else combinations_3x3 if size == 3 else [(0, 0)]\n",
    "                for start_row, start_col in combinations:\n",
    "                    rows = slice(start_row, start_row + size)\n",
    "                    cols = slice(start_col, start_col + size)\n",
    "                    if np.array_equal(puzzle_a[rows, cols], puzzle_s[rows, cols]):\n",
    "                        correct_count += 1\n",
    "                    total_subpuzzles += 1\n",
    "            accuracies[f'{size}x{size}'] = correct_count / total_subpuzzles\n",
    "        score = (accuracies['1x1'] + accuracies['2x2'] + accuracies['3x3'] + accuracies['4x4']) / 4.\n",
    "        return score, accuracies\n",
    "        \n",
    "    def _probs_to_order(self, probs): # Greedily arrange the jigsaw puzzle pieces based on maximum probability.\n",
    "        order = []\n",
    "        for prob in probs:\n",
    "            prob = prob.reshape(16,16).clone()\n",
    "            indices = [-1 for _ in range(16)]\n",
    "            for _ in range(16):\n",
    "                i, j = divmod(int(prob.argmax()),16)\n",
    "                indices[i]=j\n",
    "                prob[i, :] = float('-inf')\n",
    "                prob[:, j] = float('-inf')\n",
    "            order.append(indices)\n",
    "        order = torch.LongTensor(order)\n",
    "        return order\n",
    "    \n",
    "    def _reorder_image(self, images, reorders):\n",
    "        device = images.device\n",
    "        images_reordered = []\n",
    "        for image, reorder in zip(images, reorders):\n",
    "            image = image.cpu().clone().numpy()\n",
    "            reorder = reorder.cpu().clone().numpy()\n",
    "            c, h, w = image.shape\n",
    "            block_h, block_w = h//4, w//4\n",
    "            image_src = [[0 for _ in range(4)] for _ in range(4)]\n",
    "            for idx, order in enumerate(reorder):\n",
    "                h_idx, w_idx = divmod(order,4)\n",
    "                h_idx_shuffle, w_idx_shuffle = divmod(idx, 4)\n",
    "                image_src[h_idx][w_idx] = image[:, block_h * h_idx_shuffle : block_h * (h_idx_shuffle+1), block_w * w_idx_shuffle : block_w * (w_idx_shuffle+1)]\n",
    "            image_reordered = np.concatenate([np.concatenate(image_row, -1) for image_row in image_src], -2)\n",
    "            image_reordered = torch.from_numpy(image_reordered)\n",
    "            images_reordered.append(image_reordered)\n",
    "        images_reordered = torch.stack(images_reordered).to(device)\n",
    "        return images_reordered\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_score',\n",
    "    mode='max',\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='jigsawelectra-vitgap-{epoch:02d}-{val_score:.4f}',\n",
    "    save_top_k=3,\n",
    "    save_weights_only=True\n",
    ")\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_score\", mode=\"max\", patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708eee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsaw_electra = LitJigsawElectra(model, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb67b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=100, precision='bf16-mixed', callbacks=[checkpoint_callback, earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lit_jigsaw_electra, train_dataloader, val_dataloader) # tensorboard --logdir=./lightning_logs/version_{} 로 모니터링 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = JigsawDataset(\n",
    "    df=val_df,\n",
    "    data_path='./data',\n",
    "    mode='val'\n",
    ")\n",
    "\n",
    "pred_dataset = JigsawDataset(\n",
    "    df=test_df,\n",
    "    data_path='./data',\n",
    "    mode='inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, collate_fn=JigsawCollateFn(transform, 'val'), batch_size=config['batch_size'])\n",
    "pred_dataloader = DataLoader(pred_dataset, collate_fn=JigsawCollateFn(transform, 'inference'), batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab407",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsaw_electra = LitJigsawElectra.load_from_checkpoint('./checkpoints/jigsawelectra-vitgap-epoch=99-val_score=0.9470.ckpt',model=model, config=model_config)\n",
    "lit_jigsaw_electra.inference_iter=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea310f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = trainer.predict(lit_jigsaw_electra, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07768f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_order_pred = torch.cat([order_pred for pixel_values, order_pred, order_true in val_preds]).cpu().numpy()\n",
    "val_order_true = torch.cat([order_true for pixel_values, order_pred, order_true in val_preds]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d01472",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_jigsaw_electra._get_score(val_order_true, val_order_pred) # inference_iter=1 늘린다고 좋아지지 않음. pretrained image clf 를 이용하여 선별적으로 iterative하게 하면 더 좋아질지도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6650ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(lit_jigsaw_electra, pred_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_pred = torch.cat([order_pred for pixel_values, order_pred, _ in preds]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73151901",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = order_pred+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e648e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f226d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
